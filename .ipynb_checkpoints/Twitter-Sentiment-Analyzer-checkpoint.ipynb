{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06b4e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\auggi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import itertools\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pickle\n",
    "from datetime import date\n",
    "\n",
    "#Twitter API\n",
    "import config\n",
    "consumer_key = config.consumer_key\n",
    "consumer_secret = config.consumer_secret\n",
    "access_token = config.access_token\n",
    "access_token_secret = config.access_token_secret\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78d2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Datafile\n",
    "df = pd.read_csv('IMDBDataset.csv').to_numpy()\n",
    "#temporary, cuts the dataset to 500 to save time running it when testing\n",
    "df = df[:500]\n",
    "size = int(df.size/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30605316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines functions to be used in program\n",
    "\n",
    "#Cleans HTML Input from string\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html) \n",
    "  return TextBlob(cleantext)\n",
    "\n",
    "# This is how the Naive Bayes classifier expects the input\n",
    "# removes stopwords from string\n",
    "# returns a dictionary\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return my_dict\n",
    "\n",
    "#Process Input\n",
    "def findusefulwords(arr, sentiment):\n",
    "    unique_words = {}\n",
    "    useful_words = []\n",
    "\n",
    "    for x in range (arr[0].size):\n",
    "       unique_words[x] = set(cleanhtml(arr[0][x]).split(' '))\n",
    "\n",
    "    for x in range (len(unique_words)):\n",
    "        useful_words.append((create_word_features(unique_words[x]), sentiment))\n",
    "    return useful_words\n",
    "\n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "#Search twitter API\n",
    "def searchTwitter(search_string):\n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%d/%m/%Y\")\n",
    "    tweets = tw.Cursor(api.search,\n",
    "                       q=search_string,\n",
    "                       lang=\"en\",\n",
    "                       since=d1).items(100)\n",
    "\n",
    "    all_tweets = [remove_url(tweet.text) for tweet in tweets]\n",
    "    return all_tweets\n",
    "\n",
    "#Classify Tweets\n",
    "def classifyTweets(tweets):\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7164402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate data into pos and neg reviews, find the useful words for each and save to pos_words and neg_words\n",
    "pos_reviews = []\n",
    "neg_reviews = []\n",
    "\n",
    "for x in range (size):\n",
    "    if(df[x][1] == 'positive'):\n",
    "        pos_reviews.append((df[x]))\n",
    "    elif(df[x][1] == 'negative'):\n",
    "        neg_reviews.append((df[x]))\n",
    "pos_reviews = np.array(pos_reviews).T\n",
    "pos_words = findusefulwords(pos_reviews,\"positive\")\n",
    "\n",
    "neg_reviews = np.array(neg_reviews).T\n",
    "neg_words = findusefulwords(neg_reviews,\"negative\")\n",
    "\n",
    "#split data into training and testing datasets\n",
    "train_set = neg_words[:200] + pos_words[:200]\n",
    "test_set =  neg_words[200:] + pos_words[200:]\n",
    "\n",
    "#Train the classifier and save it using pickle\n",
    "#Classify and save to pickle\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "# classifier.show_most_informative_features()\n",
    "f = open('my_classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac94e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load classifier from pickle\n",
    "f = open('my_classifier.pickle', 'rb')\n",
    "classifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840e427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "#print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3ef04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT LakersNation Lakers will lean on their championship experience to get them through this rough stretch https', 'RT TotalProSports Welp', 'RT TotalProSports Welp', 'RT TotalProSports Welp', 'RT TotalProSports Welp', 'RT TotalProSports Welp', 'RT TotalProSports Welp', 'Welp', 'RT K1erry Linnies Has Never Been Busier Business is Booming For Cincinnati Pub that Banned NBA Games Because of LeBron James https', 'RT LakersNation Lakers will lean on their championship experience to get them through this rough stretch https']\n"
     ]
    }
   ],
   "source": [
    "x = searchTwitter('Lebron')\n",
    "print(x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42011609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Clean Tweets, Classify them, Find overall sentiments, add the ablity to input search string, display overall sentiment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pythonforengineers.com/build-a-sentiment-analysis-app-with-movie-reviews/\n",
    "#https://towardsdatascience.com/twitter-sentiment-analysis-classification-using-nltk-python-fa912578614c\n",
    "#https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
